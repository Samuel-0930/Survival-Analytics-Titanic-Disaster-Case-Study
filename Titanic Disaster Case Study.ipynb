{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.788162Z","iopub.execute_input":"2025-01-26T11:57:21.788573Z","iopub.status.idle":"2025-01-26T11:57:21.796532Z","shell.execute_reply.started":"2025-01-26T11:57:21.788542Z","shell.execute_reply":"2025-01-26T11:57:21.795558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How a Data Scientist Beat the Odds\n\n이것은 전형적인 문제로, 이진 사건의 결과를 예측하는 문제입니다. 간단히 말해, 어떤 일이 일어났거나 일어나지 않았다는 뜻입니다. 예를 들어, 시험에 합격했거나 불합격했거나, 어떤 게임에서 이겼거나 졌거나, 입학했거나 입학하지 못했거나 하는 경우입니다. 이러한 이진 사건은 비즈니스에서 고객 이탈(churn)이나 고객 유지와 같은 문제를 해결할 때 자주 다뤄집니다. 또 다른 예로는 건강 관리 분야에서 사망률이나 생존 분석을 들 수 있습니다.\n\n이진 사건은 흥미로운 특성을 가지고 있습니다. 통계적으로 보면, 무작위로 추측을 해도 50%의 정확도를 얻을 수 있다는 사실을 알고 있기 때문입니다. 즉, 특정 알고리즘을 만들거나 코드를 작성하지 않고도 우연히 맞출 확률이 50%라는 것입니다. 하지만 때때로 인간이 너무 똑똑해져서 오히려 동전 던지기보다 성능이 떨어지는 상황이 발생할 수 있습니다. 이 커널에서는 Kaggle의 “Titanic: Machine Learning from Disaster” 대회를 예시로 들어, 데이터 과학 프레임워크를 어떻게 활용해 성공적으로 예측할 수 있는지 설명하려 합니다.","metadata":{}},{"cell_type":"markdown","source":"# A Data Science Framework\n\n1. **문제를 정의하기**: 데이터 과학, 빅 데이터, 머신 러닝, 예측 분석, 비즈니스 인텔리전스, 또는 기타 유행어가 해결책이라면, 그럼 문제는 무엇일까요? “수레를 말 앞에 두지 말라”는 속담이 있습니다. 문제는 요구사항보다 먼저, 요구사항은 해결책보다 먼저, 해결책은 디자인보다 먼저, 디자인은 기술보다 먼저 있어야 합니다. 너무 자주 우리는 실제로 해결해야 할 문제를 파악하기 전에 새로운 화려한 기술, 도구, 알고리즘에 쉽게 뛰어듭니다.\n   \n2. **데이터 수집**: John Naisbitt는 1984년 그의 책 《Megatrends》에서 “우리는 데이터에 빠져 있지만, 지식은 갈망하고 있다”고 썼습니다. 즉, 데이터셋은 이미 어디엔가, 어떤 형식으로든 존재할 가능성이 큽니다. 그것은 외부 데이터일 수도, 내부 데이터일 수도 있고, 구조화된 데이터일 수도, 비구조화된 데이터일 수도 있으며, 정적이거나 스트리밍 형태일 수도, 객관적이거나 주관적일 수도 있습니다. “바퀴를 다시 발명할 필요는 없다”는 속담처럼, 중요한 것은 그것이 어디에 있는지 아는 것입니다. 다음 단계에서는 “더러운 데이터”를 “깨끗한 데이터”로 변환하는 작업을 다룰 것입니다.\n\n   \n3. **데이터 소비 준비**: 이 단계는 종종 데이터 랭글링(Data Wrangling)이라고 불리며, “야생” 데이터를 “관리 가능한” 데이터로 변환하는 필수 과정입니다. 데이터 랭글링에는 데이터 저장 및 처리용 아키텍처 구현, 품질 및 통제를 위한 데이터 거버넌스 표준 개발, 데이터 추출(예: ETL 및 웹 스크래핑), 이상치, 결측치 또는 오류 데이터를 식별하기 위한 데이터 정리가 포함됩니다.\n4. **탐색적 분석 수행**: 데이터를 다뤄본 사람이라면 누구나 알고 있는 사실은 “쓰레기를 넣으면 쓰레기가 나온다(GIGO)“는 것입니다. 따라서 데이터셋에서 잠재적인 문제, 패턴, 분류, 상관관계 및 비교를 찾기 위해 기술적 및 그래픽 통계를 활용하는 것이 중요합니다. 또한, 데이터 분류(예: 질적 데이터 vs 양적 데이터)도 올바른 가설 검정이나 데이터 모델을 선택하고 이해하는 데 중요합니다.\n5. **데이터 모델링**: 기술 통계와 추론 통계처럼, 데이터 모델링은 데이터를 요약하거나 미래의 결과를 예측할 수 있습니다. 데이터셋과 예상 결과에 따라 사용할 수 있는 알고리즘이 결정됩니다. 중요한 점은 알고리즘이 도구일 뿐, 마법의 지팡이나 은총알이 아니라는 것입니다. 당신은 여전히 적합한 도구를 선택할 줄 아는 숙련된 장인이 되어야 합니다. 비유하자면, 누군가에게 필립스 드라이버를 달라고 했을 때, 그들이 일자 드라이버나 심지어 망치를 건네주는 것과 같습니다. 최선의 경우, 이는 완전히 이해하지 못하고 있다는 것을 보여주는 것이고, 최악의 경우, 프로젝트를 불가능하게 만듭니다. 데이터 모델링도 마찬가지입니다. 잘못된 모델은 최선의 경우 성능이 저조하고, 최악의 경우 잘못된 결론(실행 가능한 정보로 사용됨)을 도출할 수 있습니다.\n6. **데이터 모델 검증 및 구현**: 데이터를 일부 학습시킨 후, 이제 모델을 테스트할 시간입니다. 이는 모델이 과적합(overfitting)되지 않았거나, 선택된 데이터의 부분집합에 너무 특화되어 다른 부분집합에 정확히 맞지 않는지 확인하는 데 도움이 됩니다. 이 단계에서 우리는 모델이 데이터셋에 대해 과적합(overfitting), 일반화(generalization), 또는 과소적합(underfitting)했는지 판단합니다.\n7. **최적화 및 전략화**: 이것은 “바이오닉 맨” 단계로, 이전보다 더 나은, 더 강력한, 더 빠른 모델을 만들기 위해 다시 프로세스를 반복하는 단계입니다. 데이터 과학자로서의 전략은 개발 작업과 애플리케이션 관련 작업을 아웃소싱하여, 추천과 설계에 더 많은 시간을 할애하는 것입니다. 아이디어를 패키징할 수 있게 되면, 이것이 바로 당신의 “통화 환율”이 됩니다.","metadata":{}},{"cell_type":"markdown","source":"# Step 1: 문제 정의하기\n\n이 프로젝트에서 문제 정의는 이미 제공된 상태입니다. 주어진 문제는 타이타닉 호의 승객들이 생존했는지 아닌지를 예측하는 알고리즘을 개발하는 것입니다.\n\n----\n\n**프로젝트 요약**: RMS 타이타닉의 침몰은 역사상 가장 악명 높은 난파 사고 중 하나입니다. 1912년 4월 15일, 타이타닉은 첫 항해 중 빙산과 충돌하여 2224명의 승객과 승무원 중 1502명이 사망했습니다. 이 참사는 국제 사회에 큰 충격을 주었으며, 선박에 대한 더 나은 안전 규정을 도입하는 계기가 되었습니다.\n\n이 사고로 인해 이렇게 많은 인명이 희생된 이유 중 하나는 승객과 승무원을 위한 구명보트가 충분하지 않았기 때문입니다. 침몰 사고에서 생존한 사람들은 어느 정도 운이 따르기도 했지만, 일부 사람들은 다른 사람들보다 더 생존할 가능성이 높았습니다. 예를 들어, 여성, 어린이, 상류층 승객들이 더 많이 생존한 경향이 있었습니다.\n\n이번 도전 과제에서는 어떤 종류의 사람들이 생존할 가능성이 높은지 분석을 완료하는 것을 목표로 합니다. 특히, 기계 학습의 도구를 적용하여 어떤 승객이 이 비극에서 생존했을지 예측하는 작업을 수행합니다.","metadata":{}},{"cell_type":"markdown","source":"# Step 2: 데이터 수집\n이 데이터셋은 Kaggle의 Titanic: Machine Learning from Disaster에서 제공됩니다. ","metadata":{}},{"cell_type":"markdown","source":"# Step 3: 데이터 소비 준비\n2단계가 이미 제공되었으므로, 3단계도 제공된 것입니다. 따라서 데이터 아키텍처, 거버넌스, 추출과 같은 데이터 전처리 과정은 범위 밖입니다. 따라서 이번 프로젝트에서는 데이터 클리닝만이 범위에 포함됩니다.","metadata":{}},{"cell_type":"markdown","source":"### 3.1 라이브러리 불러오기\n다음 코드는 Python 3.x로 작성되었습니다. 라이브러리는 필요한 작업을 수행하는 미리 작성된 기능을 제공합니다. 즉, 열 줄의 코드를 작성할 필요가 있을 때, 한 줄로 작성할 수 있기 때문입니다.","metadata":{}},{"cell_type":"code","source":"# 패키지 로드\nimport pandas as pd\nimport matplotlib\nimport numpy as np\nimport scipy as sp\nimport IPython \nfrom IPython.display import display # Jupyter 노트북에서 데이터프레임을 예쁘게 출력하기\nimport sklearn\n\n# 기타 라이브러리\nimport random\nimport time\n\n# 경고 무시\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.798115Z","iopub.execute_input":"2025-01-26T11:57:21.798482Z","iopub.status.idle":"2025-01-26T11:57:21.817006Z","shell.execute_reply.started":"2025-01-26T11:57:21.798446Z","shell.execute_reply":"2025-01-26T11:57:21.815914Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.11 Load Data Modelling Libraries\n우리는 인기 있는 scikit-learn 라이브러리를 사용하여 머신러닝 알고리즘을 개발할 것입니다. sklearn에서 알고리즘은 Estimator라고 불리며, 각각 고유한 클래스에 구현됩니다. 데이터 시각화를 위해서는 matplotlib와 seaborn 라이브러리를 사용할 것입니다. 아래는 자주 사용되는 클래스들입니다.","metadata":{}},{"cell_type":"code","source":"# 일반적인 모델 알고리즘\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# 일반적인 모델링 관련 도구\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection # 중요한 특성(피처)을 선택하는 기능을 제공\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# 시각화 도구\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n# 시각화 Configuration\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12, 8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.819206Z","iopub.execute_input":"2025-01-26T11:57:21.819514Z","iopub.status.idle":"2025-01-26T11:57:21.833862Z","shell.execute_reply.started":"2025-01-26T11:57:21.819478Z","shell.execute_reply":"2025-01-26T11:57:21.832797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 데이터와 만나서 인사\n이 단계는 데이터와 처음 만나는 단계입니다. 데이터의 첫인상을 확인하고, 그것에 대해 조금 더 알아보는 것입니다. 데이터는 어떤 모습일까요 (데이터 타입과 값)? 데이터를 움직이게 하는 것은 무엇인가요 (독립 변수/특징 변수)? 목표는 무엇인가요 (종속 변수/타겟 변수)? 이 과정을 첫 데이트에 비유할 수 있습니다. 너무 서두르지 말고, 데이터를 깊이 파헤치기 전에 차근차근 알아가세요.\n\n이 단계를 시작하려면 먼저 데이터를 불러옵니다. 그 후 info()와 sample() 함수를 사용하여 변수의 데이터 타입(예: 질적 변수와 양적 변수)을 빠르게 파악합니다.\n\n----\n\n1. *Survived* 변수는 우리의 결과 변수 또는 종속 변수입니다. 이는 이진 명목형 데이터 타입으로, 생존한 경우 1, 생존하지 않은 경우 0으로 표시됩니다. 나머지 변수들은 잠재적인 예측 변수 또는 독립 변수입니다. **중요한 점은, 더 많은 예측 변수가 더 나은 모델을 만든다고 할 수 없다는 것입니다. 중요한 것은 바로 “적절한 변수”를 선택하는 것입니다.**\n\n2. *PassengerID와* *Ticket* 변수는 결과 변수에 영향을 미치지 않는 임의의 고유 식별자라고 가정됩니다. 따라서 이 변수들은 분석에서 제외될 것입니다.\n\n3. *Pclass* 변수는 티켓 클래스에 대한 순서형 데이터 타입으로, 사회경제적 지위(SES)를 나타내는 지표입니다. 1은 상류층, 2는 중류층, 3은 하류층을 의미합니다.\n\n4. *Name* 변수는 명목형 데이터 타입입니다. 이 변수는 제목에서 성별을 유추하거나, 성에서 가족 크기를 파악하거나, “doctor”나 “master”와 같은 직함을 통해 사회경제적 지위(SES)를 유추하는 데 사용할 수 있습니다. 이러한 변수들이 이미 존재하므로, 우리는 이를 활용하여 “master”와 같은 직함이 생존 여부에 영향을 미치는지 확인해보겠습니다.\n\n5. *Sex*와 *Embarked* 변수는 명목형 데이터 타입입니다. 이 변수들은 수학적 계산을 위해 더미 변수(dummy variables)로 변환될 것입니다.\n\n6. *Age*와 *Fare* 변수는 연속적인 정량적 데이터 타입입니다.\n\n7. *SibSp*는 탑승한 형제/배우자의 수를 나타내고, *Parch*는 탑승한 부모/자녀의 수를 나타냅니다. 두 변수 모두 이산적인 정량적 데이터 타입입니다. 이를 활용하여 가족 크기를 나타내는 새로운 변수로 만들 수 있으며, 하나의 독립 변수로 사용할 수 있습니다.\n\n8. *Cabin* 변수는 사건이 발생한 위치와 갑판 수준에서의 사회경제적 지위를 추정하는 데 사용될 수 있는 명목형 데이터 타입입니다. 그러나 많은 결측값이 존재하기 때문에 분석에 유용한 정보를 추가하지 않으므로 제외됩니다.","metadata":{}},{"cell_type":"code","source":"data_raw = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# 데이터셋은 보통 3개의 부분으로 나누어야 합니다: 학습(train), 테스트(test), 그리고 (최종) 검증(validation)\n# 제공된 테스트 파일은 대회 제출을 위한 검증 파일입니다. \n# 우리는 이후 섹션에서 학습 데이터를 학습과 테스트 데이터로 나누게 됩니다.\ndata_validate = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# 데이터를 다루기 위해 복사본을 생성합니다. \n# 파이썬에서 할당 또는 등호(=)는 참조를 전달하므로, 값을 복사하려면 copy() 함수를 사용해야 합니다.\n\ndata_play = data_raw.copy(deep = True) # deep = True는 깊은 복사를 의미\n\ndata_all = [data_play, data_validate]\n\n# 데이터 미리보기\nprint(data_raw.info())\ndisplay(data_raw.sample(10)) # 랜덤하게 10개를 추출","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.835286Z","iopub.execute_input":"2025-01-26T11:57:21.835606Z","iopub.status.idle":"2025-01-26T11:57:21.877777Z","shell.execute_reply.started":"2025-01-26T11:57:21.835545Z","shell.execute_reply":"2025-01-26T11:57:21.876786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.21 데이터 정리의 4C: 수정(Correcting), 완성(Completing), 생성(Creating), 변환(Converting)\n이 단계에서는 데이터를 정리하기 위해 다음 작업을 수행합니다:\n\t1.\t이상치와 오류 값을 수정하고,\n\t2.\t누락된 정보를 채우며,\n\t3.\t분석을 위한 새로운 특성을 생성하고,\n\t4.\t계산과 표현에 적합한 형식으로 필드를 변환합니다.\n\n-----\n\n1.\t**수정(Correcting)**: 데이터를 검토한 결과, 비정상적이거나 허용되지 않는 입력값은 보이지 않습니다. 하지만 나이(Age)와 요금(Fare) 변수에서 잠재적인 이상치가 발견될 수 있습니다. 다만, 이 값들이 합리적인 범위에 있으므로, 데이터셋에서 포함 또는 제외 여부를 결정하기 위해 탐색적 분석(EDA)을 완료한 후 조치를 취할 예정입니다. 만약 값이 비합리적이라면(예: 나이가 80이 아닌 800으로 표시된 경우), 이를 지금 바로 수정하는 것이 적절합니다. 그러나 데이터를 원본 값에서 수정할 때는 신중해야 합니다. 원본 값을 유지하는 것이 정확한 모델을 생성하는 데 필요할 수 있기 때문입니다.\n  \n3.\t**보완(Completing)**: Age, Cabin, 그리고 Embarked 필드에 결측값(null values)이나 누락된 데이터가 존재합니다. 결측값은 문제가 될 수 있는데, 일부 알고리즘은 결측값을 처리하는 방법을 알지 못해 실행에 실패할 수 있기 때문입니다. 반면, 의사결정 나무와 같은 일부 알고리즘은 결측값을 처리할 수 있습니다. 따라서 여러 모델을 비교하고 대조할 예정이므로, 모델링을 시작하기 전에 결측값을 처리하는 것이 중요합니다. 결측값을 처리하는 두 가지 일반적인 방법은, 레코드를 삭제하거나 합리적인 값을 사용해 누락된 값을 채우는 것입니다. 전체 레코드의 많은 비율을 차지하지 않는 이상, 단순히 레코드를 삭제하는 것은 권장되지 않습니다. 대신, 결측값을 대체하는 것이 가장 좋습니다. 질적 데이터(qualitative data)의 기본적인 처리 방법은 최빈값(mode)을 사용해 대체하는 것입니다. 양적 데이터(quantitative data)의 경우 평균(mean), 중앙값(median), 또는 평균 + 무작위 표준편차(randomized standard deviation)를 사용하여 대체할 수 있습니다. 중간 수준의 방법으로는 특정 기준에 따라 기본 방법을 사용하는 것입니다. 예를 들어, 클래스별 평균 나이를 사용하거나 요금과 사회경제적 지위(SES)에 따라 탑승 항구를 추정하는 방식입니다. 더 복잡한 방법도 존재하지만, 이를 적용하기 전에 기본 모델과 비교하여 복잡성이 실제로 가치를 더하는지 확인하는 것이 중요합니다. 이 데이터셋의 경우, Age는 중앙값으로 대체하고, Cabin 속성은 제거하며, Embarked는 최빈값으로 대체합니다. 이후 모델의 반복적인 개선 과정에서 이러한 결정이 모델의 정확도를 향상시키는지 여부를 확인하며 수정할 수 있습니다.\n\n3.\t**생성(Creating)**: 특성 엔지니어링(Feature Engineering)은 기존의 특성(features)을 활용해 새로운 특성을 만들어내고, 이를 통해 결과를 예측하는 데 도움이 되는 새로운 신호(signals)가 있는지 확인하는 과정입니다. 이 데이터셋에서는 생존에 영향을 미쳤는지 여부를 확인하기 위해 ‘호칭(Title)’ 특성을 생성할 것입니다.\n\n4.\t**변환(Converting)**: 마지막 단계지만 결코 중요도가 떨어지지 않는 작업으로, 데이터 형식을 처리하는 것입니다. 여기에는 날짜나 통화 형식은 없지만, 데이터 타입 형식이 포함됩니다. 우리의 범주형 데이터(categorical data)는 객체(object)로 불러와져 수학적 계산을 수행하기 어렵게 만듭니다. 따라서 이 데이터셋에서는 객체 데이터 타입을 범주형 더미 변수(categorical dummy variables)로 변환할 것입니다.","metadata":{}},{"cell_type":"code","source":"print('Train columns with null values:\\n', data_play.isnull().sum())\nprint(\"-\" * 10)\n\nprint('Test/Validation columns with null values:\\n', data_validate.isnull().sum())\nprint(\"-\" * 10)\n\ndata_raw.describe(include = 'all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.878767Z","iopub.execute_input":"2025-01-26T11:57:21.879061Z","iopub.status.idle":"2025-01-26T11:57:21.924066Z","shell.execute_reply.started":"2025-01-26T11:57:21.879036Z","shell.execute_reply":"2025-01-26T11:57:21.923187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.22 Clean Data\n이제 어떤 데이터를 정리해야 하는지 알았으니, 코드를 실행해 봅시다.","metadata":{}},{"cell_type":"code","source":"### COMPLETE: complete or delete missing values in train and test/validation dataset\n\nfor dataset in data_all:\n    \n    # complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    # complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    # complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n\n# delete the cabin feature/column and others previously stated to exclude in train dataset\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata_play.drop(drop_column, axis=1, inplace = True)\n\nprint(data_play.isnull().sum())\nprint('-' * 10)\nprint(data_validate.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.924913Z","iopub.execute_input":"2025-01-26T11:57:21.925184Z","iopub.status.idle":"2025-01-26T11:57:21.939729Z","shell.execute_reply.started":"2025-01-26T11:57:21.925162Z","shell.execute_reply":"2025-01-26T11:57:21.938754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### CREATE: Feature Engineering for train and test/validation dataset\n\nfor dataset in data_all:\n\n    # Discrete variables\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 # Include Myself\n\n    dataset['IsAlone'] = 1 # Initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # update to no/0 if familySize is greater than 1\n\n    # quick and dirty code split title from name\n    dataset['Title'] = dataset['Name'].str.split(\",\", expand = True)[1].str.split(\".\", expand = True)[0]\n\n    # Continuous variable bins\n    # Fare Bins/Buckets using qcut or frequency bins\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    # Age Bins/Buckets using cut or value bins\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n# cleanup rare title names\n# print(data_play['Title'].value_counts())\nstat_min = 10\ntitle_names = (data_play['Title'].value_counts() < stat_min) # this will create a true false series with title name as index\n\n# apply and lambda functions are quick and dirty code to find and replace with fewer lines of code\ndata_play['Title'] = data_play['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data_play['Title'].value_counts())\nprint('-' * 10)\n\n# preview data again\ndata_play.info()\ndata_validate.info()\ndata_play.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:21.940847Z","iopub.execute_input":"2025-01-26T11:57:21.941215Z","iopub.status.idle":"2025-01-26T11:57:22.026340Z","shell.execute_reply.started":"2025-01-26T11:57:21.941153Z","shell.execute_reply":"2025-01-26T11:57:22.025456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.23 포맷 변환\n우리는 수학적 분석을 위해 범주형 데이터를 더미 변수로 변환할 것입니다. 범주형 변수를 인코딩하는 방법에는 여러 가지가 있으며, 우리는 sklearn과 pandas 함수를 사용할 것입니다.\n\n이 단계에서는 데이터 모델링을 위해 x(독립 변수/특징/설명 변수/예측 변수 등)와 y(종속 변수/타겟/결과/반응 변수 등)를 정의할 것입니다.","metadata":{}},{"cell_type":"code","source":"# CONVERT: convert objects to category using Labdel Encoder for train and test/validation dataset\n\n# code categorical data\nlabel = LabelEncoder() # from sklearn.preprocessing import LabelEncoder\n\nfor dataset in data_all:\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n# define y variable aka target/outcome\nTarget = ['Survived']\n\n# define x variables for original features aka feature selection\ndata_play_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] # pretty name/values for charts\ndata_play_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] # coded for algorithm calculation\ndata_play_xy = Target + data_play_x\nprint('Original X Y:', data_play_xy, '\\n')\n\n# define x variables for original w/bin features to remove continous variables\ndata_play_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata_play_xy_bin = Target + data_play_x_bin\nprint('Bin X Y:', data_play_xy_bin, '\\n')\n\n# define x and y variables for dummy features original\ndata_play_dummy = pd.get_dummies(data_play[data_play_x])\ndata_play_x_dummy = data_play_dummy.columns.tolist()\ndata_play_xy_dummy = Target + data_play_x_dummy\nprint('Dummy X Y:', data_play_xy_dummy, '\\n')\n\ndata_play_dummy.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:22.027343Z","iopub.execute_input":"2025-01-26T11:57:22.027732Z","iopub.status.idle":"2025-01-26T11:57:22.069923Z","shell.execute_reply.started":"2025-01-26T11:57:22.027697Z","shell.execute_reply":"2025-01-26T11:57:22.069081Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.24 Da-Double Check Cleanded Data","metadata":{}},{"cell_type":"code","source":"print('Train columns with null values: \\n', data_play.isnull().sum())\nprint(\"-\" * 10)\nprint(data_play.info())\nprint(\"-\" * 10)\n\nprint('Test/Validation columns with null values: \\n', data_validate.isnull().sum())\nprint('-' * 10)\nprint(data_validate.info())\nprint('-' * 10)\n\ndata_raw.describe(include = 'all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:22.072516Z","iopub.execute_input":"2025-01-26T11:57:22.072796Z","iopub.status.idle":"2025-01-26T11:57:22.130462Z","shell.execute_reply.started":"2025-01-26T11:57:22.072774Z","shell.execute_reply":"2025-01-26T11:57:22.129466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.25 Split Training and Testing Data\n앞서 언급한 것처럼, 제공된 테스트 파일은 실제로 대회 제출을 위한 검증 데이터입니다. 그래서 우리는 sklearn 함수를 사용하여 훈련 데이터를 두 개의 데이터셋으로 나눕니다; 75%/25% 비율로 나누게 됩니다. 이는 모델이 과적합(overfitting)되지 않도록 하는 데 중요합니다. 과적합이란 알고리즘이 특정한 데이터의 부분집합에 너무 특화되어 다른 부분집합에 대해 정확히 일반화할 수 없는 경우를 의미합니다. 중요한 점은 알고리즘이 우리가 테스트에 사용할 부분집합을 이미 보지 않았다는 것입니다. 그래야 “정답을 외우는” 방식으로 테스트 데이터를 예측하지 않게 됩니다. 우리는 sklearn의 train_test_split 함수를 사용할 것입니다. 이후 섹션에서는 데이터 모델링 비교를 위한 훈련과 테스트를 나누는 sklearn의 교차 검증(cross validation) 함수도 사용할 예정입니다.","metadata":{}},{"cell_type":"code","source":"# split train and test data with funciton defaults\n\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data_play[data_play_x_calc], data_play[Target], random_state=0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data_play[data_play_x_bin], data_play[Target], random_state=0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data_play_dummy[data_play_x_dummy], data_play[Target], random_state=0)\n\nprint('Data1 Shape: {}'.format(data_play.shape))\nprint('Train1 Shape: {}'.format(train1_x.shape))\nprint('Test1 Shape: {}'.format(test1_x.shape))\n\ntrain1_x_bin.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:22.132086Z","iopub.execute_input":"2025-01-26T11:57:22.132355Z","iopub.status.idle":"2025-01-26T11:57:22.155069Z","shell.execute_reply.started":"2025-01-26T11:57:22.132331Z","shell.execute_reply":"2025-01-26T11:57:22.154124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 4: 통계적 탐색적 분석 수행\n이제 데이터가 정리되었으므로, 기술적 통계와 그래프 통계를 사용하여 데이터를 탐색하고 변수들을 설명하고 요약할 것입니다. 이 단계에서는 특성들을 분류하고, 목표 변수와 각 변수 간의 상관 관계를 파악하게 될 것입니다.","metadata":{}},{"cell_type":"code","source":"# Discrete Variable Correlation by Survival using\n\nfor x in data_play_x:\n    if data_play[x].dtype != 'float64':\n        print('Survival Correlation by:', x)\n        print(data_play[[x, Target[0]]].groupby(x, as_index = False).mean().sort_values(by=Target[0], ascending = False))\n        print('-'*10, '\\n')\n\n# using crosstabs\npd.crosstab(data_play['Title'], data_play[Target[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:22.156117Z","iopub.execute_input":"2025-01-26T11:57:22.156480Z","iopub.status.idle":"2025-01-26T11:57:22.212747Z","shell.execute_reply.started":"2025-01-26T11:57:22.156446Z","shell.execute_reply":"2025-01-26T11:57:22.211887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# we will use seaborn graphics for multi-variable comparison\n# graph individual features by survival\n\nfig, axs = plt.subplots(2, 3, figsize = (16, 12))\n\nsns.barplot(x = 'Embarked', y='Survived', data=data_play, ax = axs[0, 0], ci=None)\nsns.barplot(x = 'Pclass', y='Survived', data=data_play, ax = axs[0, 1], ci=None)\nsns.barplot(x = 'IsAlone', y='Survived', data=data_play, ax = axs[0, 2], ci=None)\n\nsns.pointplot(x = 'FareBin', y='Survived', data=data_play, ax = axs[1, 0], ci=None)\nsns.pointplot(x = 'AgeBin', y='Survived', data=data_play, ax = axs[1, 1], ci=None)\nsns.pointplot(x = 'FamilySize', y='Survived', data=data_play, ax = axs[1, 2], ci=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:22.213673Z","iopub.execute_input":"2025-01-26T11:57:22.213982Z","iopub.status.idle":"2025-01-26T11:57:23.512541Z","shell.execute_reply.started":"2025-01-26T11:57:22.213955Z","shell.execute_reply":"2025-01-26T11:57:23.511420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# graph distributution of qualitative data: Pclass\n# we know class mattered in survival, now let's compare class and a 2nd feature\n\nfig, (axs1, axs2, axs3) = plt.subplots(1, 3, figsize = (14, 12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data_play, ax = axs1)\naxs1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data_play, split = True, ax = axs2)\naxs2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data_play, ax = axs3)\naxs3.set_title('Pclass vs Family Size Survival Comparison')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:23.513635Z","iopub.execute_input":"2025-01-26T11:57:23.513945Z","iopub.status.idle":"2025-01-26T11:57:24.484599Z","shell.execute_reply.started":"2025-01-26T11:57:23.513919Z","shell.execute_reply":"2025-01-26T11:57:24.483521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# graph distribution ofqualitative data: Sex\n# we know sex amttered insurvival, now let's compare sex and a 2nd feature\n\nfig, axs = plt.subplots(1, 3, figsize = (14,12))\n\nsns.barplot(x = 'Sex', y='Survived', hue='Embarked', data=data_play, ax=axs[0], ci=None)\naxs1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y='Survived', hue='Pclass', data=data_play, ax=axs[1], ci=None)\naxs1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y='Survived', hue='IsAlone', data=data_play, ax=axs[2], ci=None)\naxs1.set_title('Sex vs IsAlone Survival Comparison')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:24.485860Z","iopub.execute_input":"2025-01-26T11:57:24.486196Z","iopub.status.idle":"2025-01-26T11:57:25.139263Z","shell.execute_reply.started":"2025-01-26T11:57:24.486170Z","shell.execute_reply":"2025-01-26T11:57:25.138252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# more side-by-side comparisons\nfig, (axs1, axs2) = plt.subplots(1, 2, figsize=(14, 12))\n\n# how does family size factor with sex & survival compare\nsns.pointplot(x='FamilySize', y='Survived', hue='Sex', data=data_play, ax=axs1, ci=None, markers=['*', 'o'])\n\n# how does Pclass factor with sex & survival compare\nsns.pointplot(x='Pclass', y='Survived', hue='Sex', data=data_play, ax=axs2, ci=None, markers=['*', 'o'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:25.140355Z","iopub.execute_input":"2025-01-26T11:57:25.140732Z","iopub.status.idle":"2025-01-26T11:57:25.739968Z","shell.execute_reply.started":"2025-01-26T11:57:25.140696Z","shell.execute_reply":"2025-01-26T11:57:25.738935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# how does embark port factor with class, sex, and survival compare\ng = sns.FacetGrid(data_play, col='Embarked')\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=None, palette='deep')\ng.add_legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:25.741054Z","iopub.execute_input":"2025-01-26T11:57:25.741369Z","iopub.status.idle":"2025-01-26T11:57:26.945603Z","shell.execute_reply.started":"2025-01-26T11:57:25.741332Z","shell.execute_reply":"2025-01-26T11:57:26.944587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot distribution of age of passengers who survived or did not survive\ng = sns.FacetGrid(data_play, hue='Survived', aspect=4)\ng.map(sns.kdeplot, 'Age', shade=True)\ng.set(xlim=(0, data_play['Age'].max()))\ng.add_legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:26.946690Z","iopub.execute_input":"2025-01-26T11:57:26.947246Z","iopub.status.idle":"2025-01-26T11:57:27.541616Z","shell.execute_reply.started":"2025-01-26T11:57:26.947212Z","shell.execute_reply":"2025-01-26T11:57:27.540552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# histogram comparison of sex, class, and age by survival \ng = sns.FacetGrid(data_play, row='Sex', col='Pclass', hue='Survived')\ng.map(sns.kdeplot, 'Age', shade=True)\ng.set(xlim=(0, data_play['Age'].max()))\ng.add_legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:27.542886Z","iopub.execute_input":"2025-01-26T11:57:27.543262Z","iopub.status.idle":"2025-01-26T11:57:30.202942Z","shell.execute_reply.started":"2025-01-26T11:57:27.543224Z","shell.execute_reply":"2025-01-26T11:57:30.201716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    corr_matrix = df.corr()\n    \n    _ = sns.heatmap(\n        corr_matrix,\n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 },\n        mask = np.triu(corr_matrix)\n    )\n\ncorrelation_heatmap(data_play.select_dtypes(include=['number']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:30.203995Z","iopub.execute_input":"2025-01-26T11:57:30.204320Z","iopub.status.idle":"2025-01-26T11:57:30.964186Z","shell.execute_reply.started":"2025-01-26T11:57:30.204294Z","shell.execute_reply":"2025-01-26T11:57:30.963144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 5: 데이터 모델링\n\n데이터 사이언스(Data Science)는 수학(통계학, 선형대수 등), 컴퓨터 과학(프로그래밍 언어, 컴퓨터 시스템 등), 비즈니스 관리(의사소통, 주제에 대한 전문 지식 등) 간의 다학제적 융합 분야입니다. 대부분의 데이터 과학자들은 이 세 가지 중 하나의 분야에서 출발하기 때문에 특정 분야에 치우치는 경향이 있습니다. 하지만 데이터 과학은 세 개의 다리로 이루어진 의자와 같아서 어느 하나도 다른 것보다 더 중요하지 않습니다. 이 과정에서는 고급 수학 지식이 필요하지만 걱정할 필요는 없습니다. 우리는 고수준의 개요만 다룰 것이며, 이를 이 커널(Kernel)에서 다룰 예정입니다. 또한, 컴퓨터 과학 덕분에 많은 복잡한 작업이 이미 자동화되어 있습니다. 과거에는 수학이나 통계학의 대학원 수준 학위가 필요했던 문제들이 이제는 몇 줄의 코드만으로 해결됩니다. 마지막으로, 문제를 제대로 이해하고 접근하기 위해 비즈니스 감각이 필요합니다. 결국, 시각 안내견을 훈련시키는 것처럼, 머신러닝도 우리가 학습시키는 것이지 반대가 아닙니다.\n\n머신러닝(Machine Learning, ML)은 이름에서 알 수 있듯이 기계에게 생각하는 방법(how to think)을 가르치는 것이지, 무엇을 생각해야 하는지(what to think)를 가르치는 것이 아닙니다. 이 주제와 빅데이터는 수십 년 전부터 존재해 왔지만, 최근에는 진입 장벽이 낮아지면서 더 많은 기업과 전문가들에게 접근 가능해져 더욱 인기를 끌고 있습니다. 이는 장점과 단점을 모두 가지고 있습니다. 장점은 이 알고리즘들이 더 많은 사람들에게 접근 가능해졌기 때문에 현실 세계의 다양한 문제를 해결할 수 있다는 것입니다. 단점은 진입 장벽이 낮아지면서 많은 사람들이 사용하는 도구를 제대로 이해하지 못한 채로 잘못된 결론에 도달할 가능성이 높아졌다는 점입니다. 그래서 저는 단순히 무엇을 해야 하는지뿐만 아니라, 왜 그렇게 해야 하는지를 가르치는 데 초점을 맞추고 있습니다. 이전에 사용한 비유로, 누군가에게 필립 드라이버를 가져오라고 요청했을 때, 플랫헤드 드라이버나 심지어 망치를 가져오는 상황을 들 수 있습니다. 이는 최소한 제대로 이해하지 못했음을 보여주며, 최악의 경우 프로젝트를 완전히 실패로 이끌거나 잘못된 실행 가능한 정보를 적용하게 됩니다. 이제 이 점을 충분히 강조했으니(농담 아님), 무엇을 해야 하는지, 그리고 그보다 중요한 왜 해야 하는지를 알려드리겠습니다.\n\n먼저, 머신러닝의 목적은 인간의 문제를 해결하는 데 있다는 점을 이해해야 합니다. 머신러닝은 크게 세 가지로 분류됩니다: 지도 학습(Supervised Learning), 비지도 학습(Unsupervised Learning), 강화 학습(Reinforced Learning)입니다. 지도 학습은 정답이 포함된 훈련 데이터셋을 제공하여 모델을 학습시키는 방식입니다. 비지도 학습은 정답이 포함되지 않은 훈련 데이터셋을 사용하여 모델을 학습시키는 방식입니다. 강화 학습은 이전 두 가지의 혼합 형태로, 모델에 정답을 즉시 제공하지 않고 일련의 사건 이후에 정답을 제공하여 학습을 강화하는 방식입니다. 우리는 지도 학습(Supervised Learning)을 사용할 것입니다. 이는 특성과 그에 대응하는 목표 값을 제공하여 알고리즘을 학습시키는 방식입니다. 이후 동일한 데이터셋에서 새로운 하위 데이터를 제공하고, 유사한 예측 정확도를 얻기를 기대합니다.\n\n머신러닝 알고리즘은 많지만, 이를 목표 변수와 데이터 모델링 목표에 따라 분류(Classification), 회귀(Regression), 군집화(Clustering), 차원 축소(Dimensionality Reduction)의 네 가지 범주로 요약할 수 있습니다. 군집화와 차원 축소는 나중에 다룰 것이며, 이번에는 분류와 회귀에 초점을 맞출 것입니다. 일반적으로 연속형 목표 변수는 회귀 알고리즘이 필요하고, 이산형 목표 변수는 분류 알고리즘이 필요합니다. 참고로, 로지스틱 회귀(Logistic Regression)는 이름에 회귀가 포함되어 있지만 사실은 분류 알고리즘입니다. 이번 문제는 승객이 생존했는지 생존하지 못했는지(생존 여부)를 예측하는 것이므로, 이는 이산형 목표 변수에 해당합니다. 따라서 우리는 sklearn 라이브러리에서 분류 알고리즘을 사용해 분석을 시작할 것입니다. 이후 교차 검증(Cross Validation)과 평가 지표(Scoring Metrics)를 사용해 알고리즘의 성능을 비교하고 순위를 매길 것입니다. (평가 방법은 이후 섹션에서 다룰 예정입니다.)\n\n**Machine Learning Classification Algorithms**:\n- Ensemble Methods\n- Generalized Linear Models (GLM)\n- Naive Bayes\n- Nearest Neighbors\n- Support Vector Machines (SVM)\n- Decision Trees\n- Discriminant Analysis\n\n## 데이터 사이언스 101: 머신러닝 알고리즘(MLA) 선택 방법\n\n**중요**: 데이터 모델링에서 초보자들이 가장 자주 묻는 질문은 “최고의 머신러닝 알고리즘은 무엇인가요?“입니다. 이에 대해 초보자는 머신러닝의 No Free Lunch Theorem(NFLT, 공짜 점심 정리)를 배워야 합니다. 간단히 말하면, NFLT는 모든 상황과 모든 데이터셋에서 가장 잘 작동하는 슈퍼 알고리즘은 존재하지 않는다는 것을 말합니다. 따라서 가장 좋은 접근법은 여러 머신러닝 알고리즘(MLA)을 시도하고, 튜닝하며, 특정 상황에 맞게 비교하는 것입니다.\n\n이와 관련하여 알고리즘을 비교하는 몇 가지 훌륭한 연구가 이루어졌습니다.\n- Caruana & Niculescu-Mizil (2006): MLA 비교에 대한 강의 영상(MLA comparisons)\n- Ogutu et al. (2011): NIH에서 유전체 선택(Genomic Selection)을 위해 수행한 연구\n- Fernandez-Delgado et al. (2014): 17개의 알고리즘 계열에서 179개의 분류기를 비교\n- Thoma (2016): sklearn 알고리즘 비교\n\n또한, “더 많은 데이터가 더 나은 알고리즘을 이긴다”는 학파도 있습니다.\n\n그렇다면 초보자는 어디서부터 시작해야 할까요?\n초보자에게는 트리(Tree), 배깅(Bagging), 랜덤 포레스트(Random Forests), 그리고 부스팅(Boosting)을 추천합니다. 이들은 기본적으로 결정 트리(Decision Tree)를 다양한 방식으로 구현한 것으로, 배우고 이해하기 가장 쉬운 개념입니다. 또한, SVC(Support Vector Classifier)와 같은 알고리즘보다 튜닝이 더 쉽다는 장점이 있습니다(튜닝은 다음 섹션에서 다룹니다).\n\n아래에서는 여러 머신러닝 알고리즘을 실행하고 비교하는 방법의 개요를 설명하겠습니다. 하지만 이 커널의 나머지 부분에서는 결정 트리와 그 파생 알고리즘을 통한 데이터 모델링 학습에 초점을 맞출 것입니다.","metadata":{}},{"cell_type":"code","source":"# Machine Learning Algorithm (MLA) Selection and Initialization\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier(),\n\n    # LightGBM\n    LGBMClassifier(verbose=-1)\n]\n\n# split dataset in cross-validation with this splitter class\n# run model 10x with 60/30 split intentionally leaving out 10%\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 2025)\n\n# Create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n# create table to compare MLA predictions\nMLA_predict = data_play[Target]\n\n# index through MLA and save performance to table\nfor row_index, alg in enumerate(MLA):\n\n    # set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n    # score model with cross validation\n    cv_results = model_selection.cross_validate(alg, data_play[data_play_x_bin], data_play[Target], cv = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n\n# print and sort table\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace=True)\nMLA_compare","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:30.965293Z","iopub.execute_input":"2025-01-26T11:57:30.965675Z","iopub.status.idle":"2025-01-26T11:57:42.760946Z","shell.execute_reply.started":"2025-01-26T11:57:30.965637Z","shell.execute_reply":"2025-01-26T11:57:42.760002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color='g')\n\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:42.761855Z","iopub.execute_input":"2025-01-26T11:57:42.762103Z","iopub.status.idle":"2025-01-26T11:57:43.322960Z","shell.execute_reply.started":"2025-01-26T11:57:42.762082Z","shell.execute_reply":"2025-01-26T11:57:43.321972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.11 하이퍼 파라미터 튜닝 모델\n우리가 sklearn의 결정 트리(DT) 분류기를 사용할 때, 함수의 기본값을 모두 그대로 사용했습니다. 이는 다양한 하이퍼파라미터 설정이 모델 정확도에 어떤 영향을 미칠지 확인할 기회를 제공합니다.\n\n하지만 모델을 조정하려면, 먼저 모델을 이해해야 합니다. 그래서 이전 섹션에서 예측이 어떻게 작동하는지 보여주기 위해 시간을 들였던 것입니다. 이제 결정 트리(DT) 알고리즘에 대해 조금 더 배워보겠습니다.\n\n>**결정 트리의 장점은 다음과 같습니다:**\n* 이해하고 해석하기 간단합니다. 트리는 시각화할 수 있습니다.\n* 데이터 준비가 거의 필요하지 않습니다. 다른 기법들은 종종 데이터 정규화, 더미 변수 생성, 결측값 제거가 필요하지만, 이 모듈은 결측값을 지원하지 않습니다.\n* 트리 사용 비용(즉, 데이터 예측)은 트리를 훈련하는 데 사용된 데이터 포인트 수에 대해 로그형입니다.\n* 숫자형 데이터와 범주형 데이터 모두 처리할 수 있습니다. 다른 기법들은 보통 한 종류의 변수만 분석하는 데 특화되어 있습니다. 더 많은 정보를 원하면 알고리즘을 참조하세요.\n* 다중 출력 문제를 처리할 수 있습니다.\n* 화이트 박스 모델을 사용합니다. 주어진 상황이 모델에서 관찰 가능하면, 해당 조건에 대한 설명은 부울 논리를 통해 쉽게 설명할 수 있습니다. 반면, 블랙 박스 모델(예: 인공 신경망)에서는 결과를 해석하기 더 어려울 수 있습니다.\n* 통계적 검정을 사용하여 모델을 검증할 수 있습니다. 이를 통해 모델의 신뢰성을 평가할 수 있습니다.\n* 모델의 가정이 실제 데이터에서 약간 위배되더라도 잘 수행됩니다.\n\n>**결정 트리의 단점은 다음과 같습니다:**\n* 결정 트리 학습기는 과도하게 복잡한 트리를 생성할 수 있으며, 이는 데이터를 잘 일반화하지 못합니다. 이를 과적합(overfitting)이라고 합니다. 이 문제를 피하려면 가지치기(pruning, 현재 지원되지 않음), 리프 노드에서 요구되는 최소 샘플 수 설정 또는 트리의 최대 깊이 설정과 같은 메커니즘이 필요합니다.\n* 결정 트리는 불안정할 수 있습니다. 데이터의 작은 변화가 완전히 다른 트리를 생성할 수 있기 때문입니다. 이 문제는 앙상블 내에서 결정 트리를 사용하여 완화할 수 있습니다.\n* 최적의 결정 트리를 학습하는 문제는 여러 최적성 측면에서 NP-완전 문제로 알려져 있으며, 간단한 개념에 대해서도 마찬가지입니다. 따라서 실제 결정 트리 학습 알고리즘은 각 노드에서 로컬 최적 결정을 내리는 탐욕 알고리즘(greedy algorithm)과 같은 휴리스틱 알고리즘에 기반합니다. 이러한 알고리즘은 전역적으로 최적의 결정 트리를 반환한다고 보장할 수 없습니다. 이는 앙상블 학습기를 사용하여 여러 트리를 훈련시키고, 특징과 샘플을 교체 가능성 있는 방식으로 랜덤 샘플링하여 완화할 수 있습니다.\n* 결정 트리가 쉽게 표현하지 못하는 학습이 어려운 개념이 있습니다. 예를 들어 XOR, 패리티(parity) 문제 또는 멀티플렉서(multiplexer) 문제가 그렇습니다.\n* 결정 트리 학습기는 일부 클래스가 우세할 경우 편향된 트리를 생성할 수 있습니다. 따라서 결정 트리로 모델을 학습시키기 전에 데이터셋을 균형 맞추는 것이 좋습니다.","metadata":{}},{"cell_type":"code","source":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data_play[data_play_x_bin], data_play[Target], cv  = cv_split)\ndtree.fit(data_play[data_play_x_bin], data_play[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params()) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\nprint('-'*10)\n\n\n#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],   \n              'max_depth': [2,4,6,8,10,None], \n              'min_samples_split': [2,5,10,.03,.05], \n              'min_samples_leaf': [1,5,10,.03,.05], \n              'random_state': [0] \n             }\n\n#print(list(model_selection.ParameterGrid(param_grid)))\n\n#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\ntune_model = model_selection.RandomizedSearchCV(tree.DecisionTreeClassifier(), param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data_play[data_play_x_bin], data_play[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n\n\n#duplicates gridsearchcv\n#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n#print('AFTER DT Parameters: ', tune_model.best_params_)\n#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n#print('-'*10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:43.324138Z","iopub.execute_input":"2025-01-26T11:57:43.324434Z","iopub.status.idle":"2025-01-26T11:57:44.417586Z","shell.execute_reply.started":"2025-01-26T11:57:43.324409Z","shell.execute_reply":"2025-01-26T11:57:44.416450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.12 Tune Model with Feature Selection\n시작할 때 말했듯이, 더 많은 예측 변수가 더 나은 모델을 만들지는 않지만, 올바른 예측 변수가 중요합니다. 따라서 데이터 모델링의 또 다른 단계는 특징 선택(feature selection)입니다. Sklearn에는 여러 가지 방법이 있지만, 우리는 교차 검증(CV)과 함께 재귀적 특징 제거(RFE)를 사용할 것입니다.","metadata":{}},{"cell_type":"code","source":"#base model\nprint('BEFORE DT RFE Training Shape Old: ', data_play[data_play_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data_play[data_play_x_bin].columns.values)\n \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data_play[data_play_x_bin], data_play[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\nX_rfe = data_play[data_play_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data_play[X_rfe], data_play[Target], cv  = cv_split)\n\n#print(dtree_rfe.grid_scores_)\nprint('AFTER DT RFE Training Shape New: ', data_play[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n \nprint(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#tune rfe model\nrfe_tune_model = model_selection.RandomizedSearchCV(tree.DecisionTreeClassifier(), param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data_play[X_rfe], data_play[Target])\n\n#print(rfe_tune_model.cv_results_.keys())\n#print(rfe_tune_model.cv_results_['params'])\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n#print(rfe_tune_model.cv_results_['mean_train_score'])\n#print(rfe_tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:44.420606Z","iopub.execute_input":"2025-01-26T11:57:44.420963Z","iopub.status.idle":"2025-01-26T11:57:45.640241Z","shell.execute_reply.started":"2025-01-26T11:57:44.420933Z","shell.execute_reply":"2025-01-26T11:57:45.639248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data_play_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:45.641342Z","iopub.execute_input":"2025-01-26T11:57:45.641679Z","iopub.status.idle":"2025-01-26T11:57:45.760828Z","shell.execute_reply.started":"2025-01-26T11:57:45.641653Z","shell.execute_reply":"2025-01-26T11:57:45.759797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = rfe_tune_model.predict(data_validate[X_rfe])\n\ndata_validate['Survived'] = pred\n\nsubmission = data_validate[['PassengerId', 'Survived']]\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:57:45.761797Z","iopub.execute_input":"2025-01-26T11:57:45.762118Z","iopub.status.idle":"2025-01-26T11:57:45.773092Z","shell.execute_reply.started":"2025-01-26T11:57:45.762092Z","shell.execute_reply":"2025-01-26T11:57:45.772033Z"}},"outputs":[],"execution_count":null}]}